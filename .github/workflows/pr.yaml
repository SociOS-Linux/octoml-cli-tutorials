name: PR Tests

on:
  pull_request:
    branches: [ main ]

env:
  OCTOML_EMPLOYEE: 1
  OCTOML_AGREE_TO_TERMS: 1
  OCTOML_TELEMETRY: false
  OCTOML: /home/runner/work/octoml-cli-tutorials/octoml-cli-tutorials/target/debug/octoml
  RUST_BACKTRACE: full


jobs:
  lint:
    name: shellcheck
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
    - uses: actions/checkout@v2
    - name: Run Shellcheck
      uses: ludeeus/action-shellcheck@1.1.0
  runtest:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:

    # Setup the working directory
    - uses: actions/checkout@v2
    - name: Make parent dirs for octoml-bin
      run: mkdir -p target/debug
    - name: Download octoml-bin (tar.gz)
      run: curl --fail ${{secrets.CLI_DOWNLOAD_LINK_UBUNTU}} --output octoml.tar.gz && tar xzf octoml.tar.gz && mv octoml target/debug/octoml
    - name: Show Download
      run: ls -lR target/debug/octoml
    - name: chmod octoml bin
      run: chmod 775 target/debug/octoml
    - name: Print octoml version
      run: $OCTOML -V
    - uses: actions/setup-python@v3
      with:
        python-version: 3.8
        cache: 'pip'
    - run: pip install -r tutorials/requirements.txt

    # Download models
    - name: Cache models
      id: cache-models
      uses: actions/cache@v3
      with:
        key: cache-models-${{ hashFiles('tutorials/download_model.sh') }}
        path: |
          tutorials/question_answering/model.onnx
          tutorials/generation/gpt2-lm-head-10.onnx

    # Cache huggingface models
    - name: Cache huggingface
      uses: actions/cache@v3
      with:
        key: cache-huggingface
        path: ~/.cache/huggingface

    # Install dependencies
    - name: Run setup.sh
      run: cd tutorials && ./setup.sh
    - name: Run setup-cloud.sh
      run: cd tutorials && ./setup-cloud.sh
    - name: Run docker prune and check disk
      run: |
        docker system prune -a -f
        df -h

    # QA tutorial
    - name: Build QA model
      run: cd tutorials/question_answering && $OCTOML deploy
    - name: Docker check running
      run: sleep 4  && ps -ef | grep 8000
    - name: Execute question_answering model with native grpc
      run: cd tutorials/question_answering && python run.py --triton
    - name: Execute question_answering model with python grpc
      run: cd tutorials/question_answering && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python run.py --triton
    - name: Clean up after deployment
      run: |
        df -h
        docker kill $(docker ps -q)
        docker rmi $(docker images bert -q)
        docker system df
        df -h

    # Vision tutorial
    - name: Build vision model
      run: cd tutorials/vision && $OCTOML deploy
    - name: Docker check running
      run: sleep 4  && ps -ef | grep 8000
    - name: Execute vision model with native grpc
      run: cd tutorials/vision && python run.py --triton
    - name: Execute vision model with python grpc
      run: cd tutorials/vision && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python run.py --triton
    - name: Execute vision model with http
      run: cd tutorials/vision && python run.py --triton --protocol http --port 8000
    - name: Kill deployment
      run: docker kill $(docker ps -q)
    - name: Clean up after deployment
      run: |
        df -h
        docker kill $(docker ps -q)
        docker rmi $(docker images critterblock -q)
        docker system df
        df -h

    # Generation tutorial
    - name: Build generation model
      run: cd tutorials/generation && $OCTOML deploy
    - name: Docker check running
      run: sleep 4  && ps -ef | grep 8000
    - name: Execute generation model with native grpc
      run: cd tutorials/generation && python run.py --triton
    - name: Execute generation model with python grpc
      run: cd tutorials/generation && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python run.py --triton
    - name: Clean up after deployment
      run: |
        df -h
        docker kill $(docker ps -q)
        docker rmi $(docker images gpt2-lm-head-10 -q)
        docker system df
        df -h
